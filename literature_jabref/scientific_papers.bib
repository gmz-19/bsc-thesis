 
@InProceedings{Bissyande2013,
  author     = {Bissyandé, Tegawendé F. and Lo, David and Jiang, Lingxiao and Réveillère, Laurent and Klein, Jacques and Traon, Yves Le},
  booktitle  = {2013 {IEEE} 24th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
  title      = {Got issues? {Who} cares about it? {A} large scale investigation of issue trackers from {GitHub}},
  year       = {2013},
  month      = nov,
  note       = {ZSCC: NoCitationData[s0] ISSN: 2332-6549},
  pages      = {188--197},
  abstract   = {Feedback from software users constitutes a vital part in the evolution of software projects. By filing issue reports, users help identify and fix bugs, document software code, and enhance the software via feature requests. Many studies have explored issue reports, proposed approaches to enable the submission of higher-quality reports, and presented techniques to sort, categorize and leverage issues for software engineering needs. Who, however, cares about filing issues? What kind of issues are reported in issue trackers? What kind of correlation exist between issue reporting and the success of software projects? In this study, we address the need for answering such questions by performing an empirical study on a hundred thousands of open source projects. After filtering relevant trackers, the study used about 20,000 projects. We investigate and answer various research questions on the popularity and impact of issue trackers.},
  doi        = {10.1109/ISSRE.2013.6698918},
  file       = {:Got_issues_Who_cares_about_it_A_large_scale_investigation_of_issue_trackers_from_GitHub.pdf:PDF},
  groups     = {GitHub},
  issn       = {2332-6549},
  keywords   = {Software, Communities, Correlation, Measurement, Computer bugs, Encoding, Educational institutions},
  shorttitle = {Got issues?},
  url        = {https://ieeexplore.ieee.org/document/6698918},
}

@Article{,
  title  = {GitHut 2.0 - A SMALL PLACE TO DISCOVER LANGUAGES IN GITHUB},
  groups = {GitHub},
  url    = {https://madnight.github.io/githut/#/pull_requests/2021/4},
}

 
@Book{MilaniFard2013,
  author     = {Milani Fard, Amin and Mesbah, Ali},
  title      = {{JSNOSE}: {Detecting} javascript code smells},
  year       = {2013},
  month      = sep,
  note       = {ZSCC: 0000141},
  abstract   = {JavaScript is a powerful and flexible prototype-based scripting language that is increasingly used by developers to create interactive web applications. The language is interpreted, dynamic, weakly-typed, and has first-class functions. In addition, it interacts with other web languages such as CSS and HTML at runtime. All these characteristics make JavaScript code particularly error-prone and challenging to write and maintain. Code smells are patterns in the source code that can adversely influence program comprehension and maintainability of the program in the long term. We propose a set of 13 JavaScript code smells, collected from various developer resources. We present a JavaScript code smell detection technique called JSNOSE. Our metric-based approach combines static and dynamic analysis to detect smells in client-side code. This automated technique can help developers to spot code that could benefit from refactoring. We evaluate the smell finding capabilities of our technique through an empirical study. By analyzing 11 web applications, we investigate which smells detected by JSNOSE are more prevalent.},
  doi        = {10.1109/SCAM.2013.6648192},
  file       = {:milani_fard_jsnose__2013 - JSNOSE_ Detecting Javascript Code Smells.pdf:PDF},
  groups     = {JavaScript, Software Quality},
  journal    = {IEEE 13th International Working Conference on Source Code Analysis and Manipulation, SCAM 2013},
  shorttitle = {{JSNOSE}},
}

 
@InProceedings{Hassan2008,
  author    = {Hassan, Ahmed E.},
  booktitle = {2008 {Frontiers} of {Software} {Maintenance}},
  title     = {The road ahead for {Mining} {Software} {Repositories}},
  year      = {2008},
  month     = sep,
  note      = {ZSCC: 0000408},
  pages     = {48--57},
  abstract  = {Source control repositories, bug repositories, archived communications, deployment logs, and code repositories are examples of software repositories that are commonly available for most software projects. The mining software repositories (MSR) field analyzes and cross-links the rich data available in these repositories to uncover interesting and actionable information about software systems. By transforming these repositories from static record-keeping ones into active repositories, we can guide decision processes in modern software projects. For example, data in source control repositories, traditionally used to archive code, could be linked with data in bug repositories to help practitioners propagate complex changes and to warn them about risky code based on prior changes and bugs. In this paper, we present a brief history of the MSR field and discuss several recent achievements and results of using MSR techniques to support software research and practice. We then discuss the various opportunities and challenges that lie in the road ahead for this important and emerging field.},
  doi       = {10.1109/FOSM.2008.4659248},
  file      = {:The_road_ahead_for_Mining_Software_Repositories.pdf:PDF},
  groups    = {MSR},
  keywords  = {Software, Data mining, Computer bugs, Software systems, Software engineering, Complexity theory, History},
  url       = {https://ieeexplore.ieee.org/document/4659248},
}

 
@InProceedings{Robles2010,
  author     = {Robles, Gregorio},
  booktitle  = {2010 7th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR} 2010)},
  title      = {Replicating {MSR}: {A} study of the potential replicability of papers published in the {Mining} {Software} {Repositories} proceedings},
  year       = {2010},
  month      = may,
  note       = {ZSCC: 0000100 ISSN: 2160-1860},
  pages      = {171--180},
  abstract   = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories (MSR) (2004-2006) and now Working Conference on MSR (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that MSR authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole MSR literature and some potential solutions to lower the barriers of replicability are finally presented and discussed.},
  doi        = {10.1109/MSR.2010.5463348},
  file       = {:Replicating_MSR_A_study_of_the_potential_replicability_of_papers_published_in_the_Mining_Software_Repositories_proceedings.pdf:PDF},
  groups     = {MSR},
  issn       = {2160-1860},
  keywords   = {Software, Availability, Data mining, Conferences, Book reviews, Internet, Databases, replication, tools, public datasets, mining software repositories},
  shorttitle = {Replicating {MSR}},
  url        = {https://ieeexplore.ieee.org/document/5463348},
}

@Article{Ray2017,
  author    = {Baishakhi Ray and Daryl Posnett and Premkumar Devanbu and Vladimir Filkov},
  journal   = {Communications of the {ACM}},
  title     = {A large-scale study of programming languages and code quality in {GitHub}},
  year      = {2017},
  month     = {sep},
  number    = {10},
  pages     = {91--100},
  volume    = {60},
  doi       = {10.1145/3126905},
  file      = {:LargeScale_study_programmingLanguages_CodeQuality_Github.pdf:PDF},
  groups    = {GitHub, Software Quality},
  publisher = {Association for Computing Machinery ({ACM})},
  url       = {https://dl.acm.org/doi/10.1145/3126905},
}

@InProceedings{Kalliamvakou2014,
  author    = {Eirini Kalliamvakou and Georgios Gousios and Kelly Blincoe and Leif Singer and Daniel M. German and Daniela Damian},
  booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories - {MSR} 2014},
  title     = {The promises and perils of mining {GitHub}},
  year      = {2014},
  publisher = {{ACM} Press},
  doi       = {10.1145/2597073.2597074},
  file      = {:2014_MSR_Promises_Perils.pdf:PDF},
  groups    = {GitHub},
  url       = {https://dl.acm.org/doi/10.1145/2597073.2597074},
}

@Article{Berger2019,
  author    = {Emery D. Berger and Celeste Hollenbeck and Petr Maj and Olga Vitek and Jan Vitek},
  journal   = {{ACM} Transactions on Programming Languages and Systems},
  title     = {On the Impact of Programming Languages on Code Quality},
  year      = {2019},
  month     = {dec},
  number    = {4},
  pages     = {1--24},
  volume    = {41},
  doi       = {10.1145/3340571},
  file      = {:Impact_Programming_Languages_CodeQuality_ReproductionSTudy.pdf:PDF},
  groups    = {Software Quality},
  publisher = {Association for Computing Machinery ({ACM})},
  url       = {https://dl.acm.org/doi/10.1145/3340571},
}

 
@Misc{Yamashita,
  author   = {Yamashita, Aiko and Moonen, Leon},
  note     = {ZSCC: 0000247},
  title    = {Do code smells reflect important maintainability aspects?},
  year     = {2012},
  abstract = {Abstract—Code smells are manifestations of design flaws that can degrade code maintainability. As such, the existence of code smells seems an ideal indicator for maintainability assessments. However, to achieve comprehensive and accurate evaluations based on code smells, we need to know how well they reflect factors affecting maintainability. After identifying which maintainability factors are reflected by code smells and which not, we can use complementary means to assess the factors that are not addressed by smells. This paper reports on an empirical study that investigates the extent to which code smells reflect factors affecting maintainability that have been identified as important by programmers. We consider two sources for our analysis: (1) expert-based maintainability assessments of four Java systems before they entered a maintenance project, and (2) observations and interviews with professional developers who maintained these systems during 14 working days and implemented a number of change requests. Keywords-maintainability evaluation; code smells; I.},
  file     = {:Do_CodeSmells_reflect_important_maintainability_aspects.pdf:PDF},
  groups   = {Software Quality},
  url      = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.261.2774&rep=rep1&type=pdf},
}

 
@InProceedings{Borges2016,
  author    = {Borges, Hudson and Hora, Andre and Valente, Marco Tulio},
  booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
  title     = {Understanding the {Factors} {That} {Impact} the {Popularity} of {GitHub} {Repositories}},
  year      = {2016},
  month     = oct,
  note      = {ZSCC: 0000234},
  pages     = {334--344},
  abstract  = {Software popularity is a valuable information to modern open source developers, who constantly want to know if their systems are attracting new users, if new releases are gaining acceptance, or if they are meeting user's expectations. In this paper, we describe a study on the popularity of software systems hosted at GitHub, which is the world's largest collection of open source software. GitHub provides an explicit way for users to manifest their satisfaction with a hosted repository: the stargazers button. In our study, we reveal the main factors that impact the number of stars of GitHub projects, including programming language and application domain. We also study the impact of new features on project popularity. Finally, we identify four main patterns of popularity growth, which are derived after clustering the time series representing the number of stars of 2,279 popular GitHub repositories. We hope our results provide valuable insights to developers and maintainers, which could help them on building and evolving systems in a competitive software market.},
  doi       = {10.1109/ICSME.2016.31},
  file      = {:Understanding_the_Factors_That_Impact_the_Popularity_of_GitHub_Repositories.pdf:PDF},
  groups    = {GitHub},
  keywords  = {Software, Libraries, Organizations, Documentation, HTML, Java, GitHub, Software Popularity, Open Source software, Social coding},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7816479},
}

 
@Article{Mwendi2014,
  author  = {Mwendi, Edwin},
  journal = {Journal of Software Engineering and Applications},
  title   = {Software {Frameworks}, {Architectural} and {Design} {Patterns}},
  year    = {2014},
  month   = jan,
  note    = {ZSCC: 0000036},
  pages   = {670--678},
  volume  = {07},
  doi     = {10.4236/jsea.2014.78061},
  file    = {:mwendi_software_2014 - Software Frameworks, Architectural and Design Patterns.pdf:PDF},
  groups  = {Software Quality},
}

 
@InProceedings{Emden2012,
  author    = {van Emden, Eva and Moonen, Leon},
  booktitle = {2012 19th {Working} {Conference} on {Reverse} {Engineering}},
  title     = {Assuring software quality by code smell detection},
  year      = {2012},
  month     = oct,
  note      = {ZSCC: 0000011 ISSN: 2375-5369},
  pages     = {xix--xix},
  abstract  = {In this retrospective we will review the paper “Java Quality Assurance by Detecting Code Smells” that was published ten years ago at WCRE. The work presents an approach for the automatic detection and visualization of code smells and discusses how this approach could be used in the design of a software inspection tool. The feasibility of the proposed approach was illustrated with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. It was the first tool to automatically detect code smells in source code, and we demonstrated the application of this tool in an industrial quality assessment case study. In addition to reviewing the WCRE 2002 work, we will discuss subsequent developments in this area by looking at a selection of papers that were published in its wake. In particular, we will have a look at recent related work in which we empirically investigated the relation between code smells and software maintainability in a longitudinal study where professional developers were observed while maintaining four different software systems that exhibited known code smells. We conclude with a discussion of the lessons learned and opportunities for further research.},
  doi       = {10.1109/WCRE.2012.69},
  file      = {:Assuring_software_quality_by_code_smell_detection.pdf:PDF},
  groups    = {Software Quality},
  issn      = {2375-5369},
  keywords  = {Inspection, Java, Visualization, Software systems, Software quality, Electronic mail, software inspection, quality assurance, refactoring, code smells},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6385092},
}

@InProceedings{Baron2020,
  author    = {Marvin Mu{\~{n}}oz Bar{\'{o}}n and Marvin Wyrich and Stefan Wagner},
  booktitle = {Proceedings of the 14th {ACM} / {IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
  title     = {An Empirical Validation of Cognitive Complexity as a Measure of Source Code Understandability},
  year      = {2020},
  month     = {oct},
  publisher = {{ACM}},
  doi       = {10.1145/3382494.3410636},
  file      = {:An_Empirical_Validation_of_cognitivw_Complexity_measure_Source_code_Understandability.pdf:PDF},
  groups    = {Software Quality},
  url       = {https://dl.acm.org/doi/10.1145/3382494.3410636},
}

 
@InProceedings{Saboury2017,
  author    = {Saboury, Amir and Musavi, Pooya and Khomh, Foutse and Antoniol, Giulio},
  booktitle = {2017 {IEEE} 24th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
  title     = {An empirical study of code smells in {JavaScript} projects},
  year      = {2017},
  month     = feb,
  note      = {ZSCC: 0000055},
  pages     = {294--305},
  abstract  = {JavaScript is a powerful scripting programming language that has gained a lot of attention this past decade. Initially used exclusively for client-side web development, it has evolved to become one of the most popular programming languages, with developers now using it for both client-side and server-side application development. Similar to applications written in other programming languages, JavaScript applications contain code smells, which are poor design choices that can negatively impact the quality of an application. In this paper, we investigate code smells in JavaScript server-side applications with the aim to understand how they impact the fault-proneness of applications. We detect 12 types of code smells in 537 releases of five popular JavaScript applications (i.e., express, grunt, bower, less.js, and request) and perform survival analysis, comparing the time until a fault occurrence, in files containing code smells and files without code smells. Results show that (1) on average, files without code smells have hazard rates 65\% lower than files with code smells. (2) Among the studied smells, “Variable Re-assign” and “Assignment In Conditional statements” code smells have the highest hazard rates. Additionally, we conduct a survey with 1,484 JavaScript developers, to understand the perception of developers towards our studied code smells. We found that developers consider “Nested Callbacks”, “Variable Re-assign” and “Long Parameter List” code smells to be serious design problems that hinder the maintainability and reliability of applications. This assessment is in line with the findings of our quantitative analysis. Overall, code smells affect negatively the quality of JavaScript applications and developers should consider tracking and removing them early on before the release of applications to the public.},
  doi       = {10.1109/SANER.2017.7884630},
  file      = {:An_empirical_study_of_code_smells_in_JavaScript_projects.pdf:PDF},
  groups    = {JavaScript, Software Quality},
  keywords  = {Reactive power, Computer languages, Hazards, Context, Object oriented programming, Fault diagnosis},
  url       = {https://ieeexplore.ieee.org/document/7884630},
}

 
@Book{Yamashita2013,
  author     = {Yamashita, Aiko},
  title      = {How {Good} {Are} {Code} {Smells} for {Evaluating} {Software} {Maintainability}? {Results} from a {Comparative} {Case} {Study}},
  year       = {2013},
  month      = sep,
  note       = {ZSCC: 0000016},
  abstract   = {An advantage of code smells over traditional software measures is that the former are associated with an explicit set of refactorings to improve the existing design. Past research on code smells has emphasized the formalization and automated detection of code smells, but much less has been done to empirically investigate how good are code smells for evaluating software maintainability. This paper presents a summary of the findings in the thesis by Yamashita, which aimed at investigating the strengths and limitations of code smells for evaluating software maintainability. The study conducted comprised an outsourced maintenance project involving four Java web systems with equivalent functionality but dissimilar implementation, six software professionals, and two software companies. A main result from the study is that the usefulness of code smells differs according to the granularity level (e.g., whether the assessment is done at file or system level) and the particular operationalization of maintainability (e.g., maintainability can be measured via maintenance effort, or problems encountered during maintenance, etc). This paper summarises the most relevant findings from the thesis, discusses a series of lessons learned from conducting this study, and discusses avenues for new research in the area of code smells.},
  doi        = {10.1109/ICSM.2013.97},
  file       = {:yamashita_how_2013 - How Good Are Code Smells for Evaluating Software Maintainability_ Results from a Comparative Case Study.pdf:PDF},
  groups     = {Software Quality, MSR},
  journal    = {IEEE International Conference on Software Maintenance, ICSM},
  pages      = {571},
  shorttitle = {How {Good} {Are} {Code} {Smells} for {Evaluating} {Software} {Maintainability}?},
  url        = {https://www.researchgate.net/publication/261417156_How_Good_Are_Code_Smells_for_Evaluating_Software_Maintainability_Results_from_a_Comparative_Case_Study},
}

 
@Article{Zhang2019,
  author   = {Zhang, Jie and Li, Feng and Hao, Dan and Wang, Meng and Tang, Hao and Zhang, Lu and Harman, Mark},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {A {Study} of {Programming} {Languages} and {Their} {Bug} {Resolution} {Characteristics}},
  year     = {2019},
  month    = dec,
  note     = {ZSCC: 0000001},
  pages    = {1--1},
  volume   = {PP},
  abstract = {Bug resolution is an essential part of software development. The impact of programming language on bug resolution has been a topic of much debate. Taking Python as an example, some hold the view that bugs in the language are easy to handle because its code is easy to read and understand, while others believe that the absence of static typing leads to more bug-handling effort. This paper presents the first large-scale study that investigates the connection between programming language and bug resolution characteristics. It follows the recent trend of empirical scientific reformulation of long-standing, but hitherto anecdotal, 'great debates' about the influence of programming language and paradigm on software engineering concerns. We analyse bug resolution data from over 70 million SLOC drawn from 3 million commits to 600 GitHub projects in 10 languages. The results suggest that statistically significant differences in resolution time and patch size exist between different languages and language categories. In particular, Java bug resolution consumes less elapsed time from raise to resolve, while Ruby consumes more. We also found that patches tend to touch significantly more files for strongly typed and for static languages (as one might expect given the need to maintain type annotations). However, despite this apparent extra effort, we found evidence for a significantly lower elapsed resolution time for bug resolution committed to projects constructed from statically typed languages. This finding sheds further empirical light on the debate about the importance of static typing. Indeed, more generally, we found no evidence for any correlation between bug-resolution time and size (lines or files touched), nor any evidence for correlation with other potential confounding factors, such as problem features (e.g., size, age, and commit number) and target domain.},
  doi      = {10.1109/TSE.2019.2961897},
  file     = {:zhang_study_2019 - A Study of Programming Languages and Their Bug Resolution Characteristics.pdf:PDF},
  groups   = {Software Quality},
  url      = {https://www.researchgate.net/publication/338162224_A_Study_of_Programming_Languages_and_Their_Bug_Resolution_Characteristics},
}

 
@Book{Zheng2015,
  author     = {Zheng, Qimu and Mockus, Audris and Zhou, Minghui},
  title      = {A method to identify and correct problematic software activity data: exploiting capacity constraints and data redundancies},
  year       = {2015},
  month      = aug,
  note       = {ZSCC: 0000021},
  abstract   = {Mining software repositories to understand and improve software development is a common approach in research and practice. The operational data obtained from these repositories often do not faithfully represent the intended aspects of software development and, therefore, may jeopardize the conclusions derived from it. We propose an approach to identify problematic values based on the constraints of software development and to correct such values using data redundancies. We investigate the approach using issue and commit data of Mozilla project. In particular, we identified problematic data in four types of events and found the fraction of problematic values to exceed 10\% and rapidly rising. We found the corrected values to be 50\% closer to the most accurate estimate of task completion time. Finally, we found that the models of time until fix changed substantially when data were corrected, with the corrected data providing a 20\% better fit. We discuss how the approach may be generalized to other types of operational data to increase fidelity of software measurement in practice and in research.},
  doi        = {10.1145/2786805.2786866},
  file       = {:Method_identify_correct_problematic_software_activity.pdf:PDF},
  groups     = {Software Quality},
  pages      = {648},
  shorttitle = {A method to identify and correct problematic software activity data},
  url        = {https://dl.acm.org/doi/pdf/10.1145/2786805.2786866},
}

@InCollection{Roehm2018,
  author    = {Tobias Roehm and Daniel Veihelmann and Stefan Wagner and Elmar Juergens},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer International Publishing},
  title     = {Evaluating Maintainability Prejudices with a Large-Scale Study of Open-Source Projects},
  year      = {2018},
  month     = {dec},
  pages     = {151--171},
  doi       = {10.1007/978-3-030-05767-1_10},
  file      = {:2019_Book_SoftwareQualityTheComplexityAn.pdf:PDF},
  groups    = {Software Quality},
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-05767-1_10},
}

 
@Article{Tilkov2010,
  author     = {Tilkov, Stefan and Vinoski, Steve},
  journal    = {IEEE Internet Computing},
  title      = {Node.js: {Using} {JavaScript} to {Build} {High}-{Performance} {Network} {Programs}},
  year       = {2010},
  issn       = {1941-0131},
  month      = nov,
  note       = {ZSCC: 0000848},
  number     = {6},
  pages      = {80--83},
  volume     = {14},
  abstract   = {One of the more interesting developments recently gaining popularity in the server-side JavaScript space is Node.js. It's a framework for developing high-performance, concurrent programs that don't rely on the mainstream multithreading approach but use asynchronous I/O with an event-driven programming model.},
  doi        = {10.1109/MIC.2010.145},
  file       = {:Node.js_Using_JavaScript_to_Build_High-Performance_Network_Programs.pdf:PDF},
  groups     = {JavaScript},
  keywords   = {Sockets, Instruction sets, Programming, Servers, Libraries, Writing, Registers, Node, Node.js, functional programming, Web development, JavaScript, Internet},
  shorttitle = {Node.js},
  url        = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5617064},
}

@InProceedings{10.5555/800253.807736,
  author    = {Boehm, B. W. and Brown, J. R. and Lipow, M.},
  booktitle = {Proceedings of the 2nd International Conference on Software Engineering},
  title     = {Quantitative Evaluation of Software Quality},
  year      = {1976},
  address   = {Washington, DC, USA},
  pages     = {592–605},
  publisher = {IEEE Computer Society Press},
  series    = {ICSE '76},
  abstract  = {The study reported in this paper establishes a conceptual framework and some key initial results in the analysis of the characteristics of software quality. Its main results and conclusions are:• Explicit attention to characteristics of software quality can lead to significant savings in software life-cycle costs.• The current software state-of-the-art imposes specific limitations on our ability to automatically and quantitatively evaluate the quality of software.• A definitive hierarchy of well-defined, well-differentiated characteristics of software quality is developed. Its higher-level structure reflects the actual uses to which software quality evaluation would be put; its lower-level characteristics are closely correlated with actual software metric evaluations which can be performed.• A large number of software quality-evaluation metrics have been defined, classified, and evaluated with respect to their potential benefits, quantifiability, and ease of automation.•Particular software life-cycle activities have been identified which have significant leverage on software quality.Most importantly, we believe that the study reported in this paper provides for the first time a clear, well-defined framework for assessing the often slippery issues associated with software quality, via the consistent and mutually supportive sets of definitions, distinctions, guidelines, and experiences cited. This framework is certainly not complete, but it has been brought to a point sufficient to serve as a viable basis for future refinements and extensions.},
  file      = {:Quantitative_evaluation_of_softwareQuality.pdf:PDF},
  groups    = {Software Quality},
  keywords  = {Software quality, Quality assurance, Software measurement and evaluation, Management by objectives, Quality characteristics, Software standards, Software reliability, Software engineering, Quality metrics, Testing},
  location  = {San Francisco, California, USA},
  numpages  = {14},
  url       = {https://dl.acm.org/doi/10.5555/800253.807736},
}

 
@InProceedings{Park2016,
  author    = {Park, Joonyoung and Lim, Inho and Ryu, Sukyoung},
  booktitle = {2016 {IEEE}/{ACM} 38th {International} {Conference} on {Software} {Engineering} {Companion} ({ICSE}-{C})},
  title     = {Battles with {False} {Positives} in {Static} {Analysis} of {JavaScript} {Web} {Applications} in the {Wild}},
  year      = {2016},
  month     = may,
  note      = {ZSCC: 0000027},
  pages     = {61--70},
  abstract  = {Now that HTML5 technologies are everywhere from web services to various platforms, assuring quality of web applications becomes very important.While web application developers use syntactic checkers and type-related bug detectors, extremely dynamic features and diverse execution environments of web applications make it particularly difficult to statically analyze them leading to too many false positives.Recently, researchers have developed static analyzers for JavaScript web applications addressing quirky JavaScript language semantics and browser environments, but they lack empirical studies on the practicality of such analyzers.In this paper, we collect 30 JavaScript web applications in the wild, analyze them using SAFE, the state-of-the-art JavaScript static analyzer with bug detection, and investigate false positives in the analysis results.After manually inspecting them, we classify 7 reasons that cause the false positives: W3C APIs, browser-specific APIs, JavaScript library APIs, dynamic file loading, dynamic code generation, asynchronous calls, and others. Among them, we identify 4 cases which are the sources of false positives that we can practically reduce.Rather than striving for sound analysis with unrealistic assumptions, we choose to be intentionally unsound to analyze web applications in the real world with less false positives.Our evaluation shows that the approach effectively reduces false positives in statically analyzing web applications in the wild.},
  file      = {:Battles_with_False_Positives_in_Static_Analysis_of_JavaScript_Web_Applications_in_the_Wild.pdf:PDF},
  groups    = {JavaScript},
  keywords  = {Computer bugs, Browsers, Libraries, Detectors, Analytical models, Loading, HTML},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7883289},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Proposal\;0\;1\;0x00ffffff\;\;\;;
1 StaticGroup:Asynchronous Programming\;0\;1\;0x00ff00ff\;\;\;;
1 StaticGroup:JavaScript\;0\;1\;0xffff00ff\;\;\;;
1 StaticGroup:GitHub\;0\;0\;0xf59b20ff\;\;\;;
1 StaticGroup:MSR\;0\;1\;0xff00ffff\;\;\;;
1 StaticGroup:Software Quality\;0\;1\;0xff0000ff\;\;\;;
}
